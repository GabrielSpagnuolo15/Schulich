{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, mean_squared_error, mean_absolute_error, r2_score, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "import time # to measure how long the models take\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r\"C:\\Users\\gabri\\Downloads\\train.csv.zip\", sep=\";\")\n",
    "df_test = pd.read_csv(r\"C:\\Users\\gabri\\Downloads\\test (1).csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop_duplicates(inplace=True)\n",
    "df_test.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10), (ax11, ax12), (ax13, ax14), (ax15, ax16), (ax17, _)) = plt.subplots(9, 2, figsize=(40, 120))\n",
    "\n",
    "sns.histplot(df_train['age'], ax=ax1)\n",
    "sns.histplot(df_train['job'], ax=ax2)\n",
    "sns.histplot(df_train['marital'], ax=ax3)\n",
    "sns.histplot(df_train['education'], ax=ax4)\n",
    "sns.histplot(df_train['default'], ax=ax5)\n",
    "sns.histplot(df_train['balance'], ax=ax6)\n",
    "sns.histplot(df_train['housing'], ax=ax7)\n",
    "sns.histplot(df_train['loan'], ax=ax8)\n",
    "sns.histplot(df_train['contact'], ax=ax9)\n",
    "sns.histplot(df_train['day'], ax=ax10)\n",
    "sns.histplot(df_train['month'], ax=ax11)\n",
    "sns.histplot(df_train['duration'], ax=ax12)\n",
    "sns.histplot(df_train['campaign'], ax=ax13)\n",
    "sns.histplot(df_train['pdays'], ax=ax14)\n",
    "sns.histplot(df_train['previous'], ax=ax15)\n",
    "sns.histplot(df_train['poutcome'], ax=ax16)\n",
    "sns.histplot(df_train['y'], ax=ax17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8), (ax9, ax10), (ax11, ax12), (ax13, ax14), (ax15, ax16), (ax17, _)) = plt.subplots(9, 2, figsize=(40, 120))\n",
    "\n",
    "sns.histplot(df_test['age'], ax=ax1)\n",
    "sns.histplot(df_test['job'], ax=ax2)\n",
    "sns.histplot(df_test['marital'], ax=ax3)\n",
    "sns.histplot(df_test['education'], ax=ax4)\n",
    "sns.histplot(df_test['default'], ax=ax5)\n",
    "sns.histplot(df_test['balance'], ax=ax6)\n",
    "sns.histplot(df_test['housing'], ax=ax7)\n",
    "sns.histplot(df_test['loan'], ax=ax8)\n",
    "sns.histplot(df_test['contact'], ax=ax9)\n",
    "sns.histplot(df_test['day'], ax=ax10)\n",
    "sns.histplot(df_test['month'], ax=ax11)\n",
    "sns.histplot(df_test['duration'], ax=ax12)\n",
    "sns.histplot(df_test['campaign'], ax=ax13)\n",
    "sns.histplot(df_test['pdays'], ax=ax14)\n",
    "sns.histplot(df_test['previous'], ax=ax15)\n",
    "sns.histplot(df_test['poutcome'], ax=ax16)\n",
    "sns.histplot(df_test['y'], ax=ax17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, _)) = plt.subplots(4, 2, figsize=(20, 60))\n",
    "\n",
    "sns.boxplot(df_train['age'], ax=ax1)\n",
    "sns.boxplot(df_train['balance'], ax=ax2)\n",
    "sns.boxplot(df_train['day'], ax=ax3)\n",
    "sns.boxplot(df_train['duration'], ax=ax4)\n",
    "sns.boxplot(df_train['campaign'], ax=ax5)\n",
    "sns.boxplot(df_train['pdays'], ax=ax6)\n",
    "sns.boxplot(df_train['previous'], ax=ax7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, _)) = plt.subplots(4, 2, figsize=(20, 60))\n",
    "\n",
    "sns.boxplot(df_test['age'], ax=ax1)\n",
    "sns.boxplot(df_test['balance'], ax=ax2)\n",
    "sns.boxplot(df_test['day'], ax=ax3)\n",
    "sns.boxplot(df_test['duration'], ax=ax4)\n",
    "sns.boxplot(df_test['campaign'], ax=ax5)\n",
    "sns.boxplot(df_test['pdays'], ax=ax6)\n",
    "sns.boxplot(df_test['previous'], ax=ax7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, _)) = plt.subplots(4,2, figsize=(20,40))\n",
    "\n",
    "sns.boxplot(x='y',y='age', data=df_train, ax=ax1)\n",
    "sns.boxplot(x='y',y='balance', data=df_train, ax=ax2)\n",
    "sns.boxplot(x='y',y='day', data=df_train, ax=ax3)\n",
    "sns.boxplot(x='y',y='duration', data=df_train, ax=ax4)\n",
    "sns.boxplot(x='y',y='campaign', data=df_train, ax=ax5)\n",
    "sns.boxplot(x='y',y='pdays', data=df_train, ax=ax6)\n",
    "sns.boxplot(x='y',y='previous', data=df_train, ax=ax7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, _)) = plt.subplots(4,2, figsize=(20,40))\n",
    "\n",
    "sns.boxplot(x='y',y='age', data=df_test, ax=ax1)\n",
    "sns.boxplot(x='y',y='balance', data=df_test, ax=ax2)\n",
    "sns.boxplot(x='y',y='day', data=df_test, ax=ax3)\n",
    "sns.boxplot(x='y',y='duration', data=df_test, ax=ax4)\n",
    "sns.boxplot(x='y',y='campaign', data=df_test, ax=ax5)\n",
    "sns.boxplot(x='y',y='pdays', data=df_test, ax=ax6)\n",
    "sns.boxplot(x='y',y='previous', data=df_test, ax=ax7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('y').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('y').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_type = df_train.groupby(['y','job']).size().unstack()\n",
    "employment_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employment_type = df_test.groupby(['y','job']).size().unstack()\n",
    "employment_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marriage_status = df_train.groupby(['y','marital']).size().unstack()\n",
    "marriage_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marriage_status = df_test.groupby(['y','marital']).size().unstack()\n",
    "marriage_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_type = df_train.groupby(['y','education']).size().unstack()\n",
    "education_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_type = df_test.groupby(['y','education']).size().unstack()\n",
    "education_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_status = df_train.groupby(['y','default']).size().unstack()\n",
    "default_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_status = df_test.groupby(['y','default']).size().unstack()\n",
    "default_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_loan = df_train.groupby(['y','housing']).size().unstack()\n",
    "housing_loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_loan = df_test.groupby(['y','housing']).size().unstack()\n",
    "housing_loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_status = df_train.groupby(['y','loan']).size().unstack()\n",
    "loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_status = df_test.groupby(['y','loan']).size().unstack()\n",
    "loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_type = df_train.groupby(['y','contact']).size().unstack()\n",
    "contact_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_type = df_test.groupby(['y','contact']).size().unstack()\n",
    "contact_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_month = df_train.groupby(['y','month']).size().unstack()\n",
    "contact_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_month = df_test.groupby(['y','month']).size().unstack()\n",
    "contact_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_outcome = df_train.groupby(['y','poutcome']).size().unstack()\n",
    "marketing_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing_outcome = df_test.groupby(['y','poutcome']).size().unstack()\n",
    "marketing_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['job'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['marital'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['default'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['housing'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['loan'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['contact'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['poutcome'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_mapping = {'management':1, 'technician':2, 'entrepreneur':3, 'blue-collar':4, 'unknown':5, 'retired':6, 'admin.':7, 'services':8, 'self-employed':9, 'unemployed':10, 'housemaid':11, 'student':12}\n",
    "df_train['job'] = df_train['job'].map(job_mapping)\n",
    "\n",
    "marital_mapping = {'married':1, 'single':2, 'divorced':3}\n",
    "df_train['marital'] = df_train['marital'].map(marital_mapping)\n",
    "\n",
    "education_mapping = {'tertiary':1, 'secondary':2, 'unknown':3, 'primary':4}\n",
    "df_train['education'] = df_train['education'].map(education_mapping)\n",
    "\n",
    "default_mapping = {'no':1, 'yes':2}\n",
    "df_train['default'] = df_train['default'].map(default_mapping)\n",
    "\n",
    "housing_mapping = {'no':1, 'yes':2}\n",
    "df_train['housing'] = df_train['housing'].map(housing_mapping)\n",
    "\n",
    "loan_mapping = {'no':1, 'yes':2}\n",
    "df_train['loan'] = df_train['loan'].map(loan_mapping)\n",
    "\n",
    "contact_mapping = {'unknown':1, 'cellular':2, 'telephone':3}\n",
    "df_train['contact'] = df_train['contact'].map(contact_mapping)\n",
    "\n",
    "month_mapping = {'may':1, 'jun':2, 'jul':3, 'aug':4, 'oct':5, 'nov':6, 'dec':7, 'jan':8, 'feb':9, 'mar':10, 'apr':11, 'sep':12}\n",
    "df_train['month'] = df_train['month'].map(month_mapping)\n",
    "\n",
    "poutcome_mapping = {'unknown':1, 'failure':2, 'other':3, 'success':4}\n",
    "df_train['poutcome'] = df_train['poutcome'].map(poutcome_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_mapping = {'management':1, 'technician':2, 'entrepreneur':3, 'blue-collar':4, 'unknown':5, 'retired':6, 'admin.':7, 'services':8, 'self-employed':9, 'unemployed':10, 'housemaid':11, 'student':12}\n",
    "df_test['job'] = df_test['job'].map(job_mapping)\n",
    "\n",
    "marital_mapping = {'married':1, 'single':2, 'divorced':3}\n",
    "df_test['marital'] = df_test['marital'].map(marital_mapping)\n",
    "\n",
    "education_mapping = {'tertiary':1, 'secondary':2, 'unknown':3, 'primary':4}\n",
    "df_test['education'] = df_test['education'].map(education_mapping)\n",
    "\n",
    "default_mapping = {'no':1, 'yes':2}\n",
    "df_test['default'] = df_test['default'].map(default_mapping)\n",
    "\n",
    "housing_mapping = {'no':1, 'yes':2}\n",
    "df_test['housing'] = df_test['housing'].map(housing_mapping)\n",
    "\n",
    "loan_mapping = {'no':1, 'yes':2}\n",
    "df_test['loan'] = df_test['loan'].map(loan_mapping)\n",
    "\n",
    "contact_mapping = {'unknown':1, 'cellular':2, 'telephone':3}\n",
    "df_test['contact'] = df_test['contact'].map(contact_mapping)\n",
    "\n",
    "month_mapping = {'may':1, 'jun':2, 'jul':3, 'aug':4, 'oct':5, 'nov':6, 'dec':7, 'jan':8, 'feb':9, 'mar':10, 'apr':11, 'sep':12}\n",
    "df_test['month'] = df_test['month'].map(month_mapping)\n",
    "\n",
    "poutcome_mapping = {'unknown':1, 'failure':2, 'other':3, 'success':4}\n",
    "df_test['poutcome'] = df_test['poutcome'].map(poutcome_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_train['education'].value_counts()\n",
    "print(count)\n",
    "percent = count[3] / df_train['education'].count() * 100\n",
    "print(\"The percent is\", percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_train['contact'].value_counts()\n",
    "print(count)\n",
    "percent = count[1] / df_train['education'].count() * 100\n",
    "print(\"The percent is\", percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = df_train['poutcome'].value_counts()\n",
    "print(count)\n",
    "percent = count[1] / df_train['education'].count() * 100\n",
    "print(\"The percent is\", percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['job','marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "numerical_features = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "target = 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[categorical_features + numerical_features]\n",
    "X_test = df_test[categorical_features + numerical_features]\n",
    "y_train = df_train[target]\n",
    "y_test = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(), categorical_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "X_resampled_test, y_resampled_test = smote.fit_resample(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    'K-Nearest Neighbors': knn,\n",
    "    'Logistic Regression': log_reg\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to store the results of each model\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through list of models to compare performance\n",
    "for name, clf in classifiers.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', clf)])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_resampled_test)\n",
    "\n",
    "    # Create a classification report\n",
    "    classification = classification_report(y_resampled_test, y_pred)\n",
    "    print(classification)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    recall = recall_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    f1 = f1_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    accuracy = accuracy_score(y_resampled_test, y_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Time (s)': elapsed_time\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame for easier viewing\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids for tuning\n",
    "knn_params = {'classifier__n_neighbors': [3, 5, 7, 20, 30, 50, 100]}\n",
    "log_reg_params = {'classifier__C': [0.1, 1, 10]}\n",
    "\n",
    "params_dict = {\n",
    "    'K-Nearest Neighbors': knn_params,\n",
    "    'Logistic Regression': log_reg_params\n",
    "}\n",
    "\n",
    "# Initialize results dictionary for tuned models\n",
    "tuned_results = {}\n",
    "\n",
    "# Loop through classifiers for tuning\n",
    "for name, clf in classifiers.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', clf)])\n",
    "    \n",
    "    # Create GridSearchCV object\n",
    "    grid = GridSearchCV(pipeline, params_dict[name], cv=5)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Get the best estimator and predict\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_resampled_test)\n",
    "    \n",
    "    # Create a classification report\n",
    "    classification = classification_report(y_resampled_test, y_pred)\n",
    "    print(classification)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    recall = recall_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    f1 = f1_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    accuracy = accuracy_score(y_resampled_test, y_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Store results\n",
    "    tuned_results[name] = {\n",
    "        'Best Params': grid.best_params_,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Time (s)': elapsed_time\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame for easier viewing\n",
    "tuned_results_df = pd.DataFrame(tuned_results).T\n",
    "print(tuned_results_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(max_depth=20)\n",
    "rf = RandomForestClassifier()\n",
    "ada = AdaBoostClassifier()\n",
    "bag = BaggingClassifier()\n",
    "voting = VotingClassifier(estimators=[('lr', log_reg), ('knn', knn), ('dt', dt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esemble_models = {\n",
    "    'K-Nearest Neighbors': knn,\n",
    "    'Logistic Regression': log_reg,\n",
    "    'Decision Tree': dt,\n",
    "    'Random Forest': rf,\n",
    "    'AdaBoost': ada,\n",
    "    'Bagging': bag,\n",
    "    'Voting': voting\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to store the results of each model\n",
    "results_2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through list of models to compare performance\n",
    "for name, clf in esemble_models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', clf)])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_resampled_test)\n",
    "\n",
    "    # Create a classification report\n",
    "    classification = classification_report(y_resampled_test, y_pred)\n",
    "    print(classification)\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    recall = recall_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    f1 = f1_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    accuracy = accuracy_score(y_resampled_test, y_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Store results\n",
    "    results_2[name] = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Time (s)': elapsed_time\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame for easier viewing\n",
    "results_df_2 = pd.DataFrame(results_2).T\n",
    "print(results_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids for tuning\n",
    "knn_params = {'classifier__n_neighbors': [3, 5, 7, 20, 30, 50, 100]}\n",
    "log_reg_params = {'classifier__C': [0.1, 1, 10]}\n",
    "dt_params = {'classifier__max_depth': [10,20,30,40,50]},\n",
    "rf_params = {'classifier__n_estimators': [50, 100, 150], 'classifier__max_depth': [None, 10, 20, 30, 50]}\n",
    "ada_params = {'classifier__n_estimators': [25, 50, 75]}\n",
    "bag_params = {'classifier__n_estimators': [5, 10, 20]}\n",
    "voting_params = {'classifier__voting': ['hard']}\n",
    "\n",
    "params_dict = {\n",
    "    'K-Nearest Neighbors': knn_params,\n",
    "    'Logistic Regression': log_reg_params,\n",
    "    'Decision Tree': dt_params,\n",
    "    'Random Forest': rf_params,\n",
    "    'AdaBoost': ada_params,\n",
    "    'Bagging': bag_params,\n",
    "    'Voting': voting_params\n",
    "}\n",
    "\n",
    "# Initialize results dictionary for tuned models\n",
    "tuned_results_2 = {}\n",
    "\n",
    "# Loop through classifiers for tuning\n",
    "for name, clf in esemble_models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', clf)])\n",
    "    \n",
    "    # Create GridSearchCV object\n",
    "    grid = GridSearchCV(pipeline, params_dict[name], cv=5)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Get the best estimator and predict\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_resampled_test)\n",
    "    \n",
    "    # Create a classification report\n",
    "    #classification = classification_report(y_test, y_pred)\n",
    "    #print(classification)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    recall = recall_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    f1 = f1_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    accuracy = accuracy_score(y_resampled_test, y_pred)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Store results\n",
    "    tuned_results_2[name] = {\n",
    "        'Best Params': grid.best_params_,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Time (s)': elapsed_time\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame for easier viewing\n",
    "tuned_results_df_2 = pd.DataFrame(tuned_results_2).T\n",
    "print(tuned_results_df_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grids for tuning\n",
    "knn_params = {'classifier__n_neighbors': [3, 5, 7, 20, 30, 50, 100]}\n",
    "log_reg_params = {'classifier__C': [0.1, 1, 10]}\n",
    "dt_params = {'classifier__max_depth': [10,20,30,40,50]},\n",
    "rf_params = {'classifier__n_estimators': [50, 100, 150], 'classifier__max_depth': [None, 10, 20, 30, 50]}\n",
    "ada_params = {'classifier__n_estimators': [25, 50, 75]}\n",
    "bag_params = {'classifier__n_estimators': [5, 10, 20]}\n",
    "voting_params = {'classifier__voting': ['soft']}\n",
    "\n",
    "params_dict = {\n",
    "    'K-Nearest Neighbors': knn_params,\n",
    "    'Logistic Regression': log_reg_params,\n",
    "    'Decision Tree': dt_params,\n",
    "    'Random Forest': rf_params,\n",
    "    'AdaBoost': ada_params,\n",
    "    'Bagging': bag_params,\n",
    "    'Voting': voting_params\n",
    "}\n",
    "\n",
    "# Initialize results dictionary for tuned models\n",
    "tuned_results_2 = {}\n",
    "\n",
    "# Loop through classifiers for tuning\n",
    "for name, clf in esemble_models.items():\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', clf)])\n",
    "    \n",
    "    # Create GridSearchCV object\n",
    "    grid = GridSearchCV(pipeline, params_dict[name], cv=5)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid.fit(X_resampled, y_resampled)\n",
    "    \n",
    "    # Get the best estimator and predict\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_resampled_test)\n",
    "    \n",
    "    # Create a classification report\n",
    "    #classification = classification_report(y_test, y_pred)\n",
    "    #print(classification)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    recall = recall_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    f1 = f1_score(y_resampled_test, y_pred, pos_label='yes')\n",
    "    accuracy = accuracy_score(y_resampled_test, y_pred)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    \n",
    "    # Store results\n",
    "    tuned_results_2[name] = {\n",
    "        'Best Params': grid.best_params_,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'Accuracy': accuracy,\n",
    "        'Time (s)': elapsed_time\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame for easier viewing\n",
    "tuned_results_df_2 = pd.DataFrame(tuned_results_2).T\n",
    "print(tuned_results_df_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "The approach I took to data processing and data cleaning was rather simple due to the nature of how the data is given. Normally the data is giving as one dataset, however here it was split into two datasets, the full dataset, and the test dataset. The important thing to note is that the test dataset is a random 10% subset of the full dataset. Since I don’t feel 10% is an arbitrary number, I did not feel comfortable removing any data points, even if I were to consider them outliers. Removing a point would throw off the ratio. Also, I feel in a business context if I were to be given a dataset and a subset for testing based on the originally dataset, altering the datasets would not be completing what I was given. If I were just given the dataset and had to make my own test dataset based on functions in python, then datapoints would be removed at my own discretion.\n",
    "\n",
    "Once that was decided, data processing was straightforward. I conducted univariate and bivariate analysis on both datasets to understand the variables. The univariate analysis showed me that there were not many outliers relative to the size of the dataset, which further solidified the decision mentioned above. The bivariate analysis done on both numerical and categorical data was to see the relationship between variables with respect to the target variable. It is good practice to do this for a more in-depth analysis, but since all variables were used in the model no further actions were taken. I will say I also included a breakdown of the target variable, and saw that there was a clear unbalance which needed to change come modelling time. After that I engaged in mapping to change the values within the categorical variables from words to numbers. Essentially doing a manual version of OneHotEncoder for both better model performance and insurance in case the OneHotEncoder function did not work. The last step before building the pipeline was checking the categorical variables with values marked ‘unknown’ to see if imputation was necessary. However, upon inspection I determined it was unnecessary given the values of approximately 4%, 28%, and 82%. The 28% and 82% I feel are too high to make changes, and if I were to change the 4% category, it would be through removing them, something I previously stated I was unwilling to do.\n",
    "\n",
    "The pipeline was made up of the categorical and numerical variables present with the target variable being ‘y’ in the dataset. The variables were transformed with the typical transformations (StandardScaler for numerical variables to help normalize them and OneHotEncoder for categorical variables to have values represent the categories). These were done to help the performance of the models. Finally, the SMOTE transformation was done to synthetically resample the data to create a balance dataset, something that was mentioned to be a problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "Even though I included Logistic Regression and k-NN for when it was both not tuned and hyper-tuned, the analysis will be done on the values generated from hyper-tuned models. I included 5 performance metrics, precision, recall, F1-score, accuracy, and time. A classification report was included, but that was to ensure the dataset was balanced. The results were as follows:\n",
    "\n",
    "Logistic Regression: When compared to the k-NN, Logistic Regression has slightly worse precision, recall, F1-score, and accuracy, but is a significantly faster model.\n",
    "\n",
    "k-NN: Logistic Regression: When compared to the k-NN, Logistic Regression has slightly better precision, recall, F1-score, and accuracy, but is a significantly slower model.\n",
    "\n",
    "If I was picking a model out of the 2, I would pick Logistic Regression since it essential generates the same caliber of results, but in a lot faster time which makes it computational less expensive. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "This was just generating the other models listed, which is present near the bottom of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "\n",
    "Much like in part 2, even though the models were conducted with both no tuning and hyper-tuning, the values of hyper-tuning will be used in the comparison since the question asked for it. The models to compare are k-NN, Logistic Regression, Random Forest, AdaBoost, Bagging, and Voting (Decision Tree, along with k-NN and Logistic Regression makes up the Voting classification). There were separate models made for Voting for both Hard and Soft classification. The rankings were as follows (best to worst):\n",
    "\n",
    "Precision: Bagging, Voting (Soft), Voting (Hard), k-NN, Logistic Regression, AdaBoost, and Random Forrest. Bagging was by far the best with 0.996 (suggesting potential overfitting), then Voting Soft and Hard were similar, then the other five models would be clumped together after a drop off.\n",
    "\n",
    "Recall: Bagging, Voting (Soft), Random Forest, Voting (Hard), k-NN, AdaBoost, and Logistic Regression. Bagging and Voting Soft were relatively similar, Random Forest and Voting Hard are similar after a slight drop, then the other 3 models are similar after a bigger drop.\n",
    "\n",
    "F1-Score: Bagging, Voting (Soft), Voting (Hard), Random Forrest, k-NN, AdaBoost, and Logistic Regression. Bagging is on its own, then the 2 Voting models are together, then after a drop the other 4 models are clumped together in terms of performance.\n",
    "\n",
    "Accuracy: Bagging, Voting (Soft), Voting (Hard), Random Forrest, k-NN, AdaBoost, and Logistic Regression. I would say Bagging is on its own, then Voting Soft on its own, then Voting Hard on its own, and finally the other 4 models clumped together in terms of performance.\n",
    "\n",
    "Time: Logistic Regression, Voting (Hard), Voting (Soft), k-NN, Bagging, AdaBoost, Random Forest. Linear Regression is on its own, then the Voting Models are clumped together, then k-NN way higher, then Bagging moderately higher, then AdaBoost moderately higher, and finally Random Forrest is higher by >400 seconds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5\n",
    "\n",
    "Question 1\n",
    "\n",
    "The performance of the ensemble models did not surprise me. They were computationally more expensive then Losistic Regression, but use in-depth methods to genereate better performance, which is what happened.\n",
    "\n",
    "Question 2\n",
    "\n",
    "Based on the values found, I would select the Voting (Soft) model. The criteria for this selection is not only the performance metrics, but computational power. The only real competition would be Bagging, but that produced numbers that were so high in terms of performance (specifically precision), that I did not want there to be the potential for overfitting. Voting (Soft) produced the second best precision and accuracy, but also had the third best time by a significant margin. This means less computational power which is good for the company. Also, Precision over recall means we valued lower false positives over false negatives. For this problem, it is better for a model to be developed where clients did make deposits that we did not think they would as opposed to the other way around, which is what happens when the lowering of false positives, and by extension precision carries more weight. Unfortunately, interpretability had to be sacrificed, but I think it was worth it.\n",
    "\n",
    "\n",
    "Question 3\n",
    "\n",
    "The criteria for this desision was mentioned in question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
